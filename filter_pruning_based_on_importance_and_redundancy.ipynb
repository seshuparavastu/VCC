{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1DOdPN3DQO_58MsIXENWuPKLxBgqVQWeX",
      "authorship_tag": "ABX9TyNQz1iag5LfBZxcTEEwWhOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarseshu123/VCC/blob/main/filter_pruning_based_on_importance_and_redundancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEheYPnImW4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb6700c-7792-4869-9eae-968f157e8c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "391/391 [==============================] - 7s 10ms/step - loss: 1.4790 - accuracy: 0.4644\n",
            "Epoch 2/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0050 - accuracy: 0.6479\n",
            "Epoch 3/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8179 - accuracy: 0.7144\n",
            "Epoch 4/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7032 - accuracy: 0.7542\n",
            "Epoch 5/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6084 - accuracy: 0.7887\n",
            "Epoch 6/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5280 - accuracy: 0.8150\n",
            "Epoch 7/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.4582 - accuracy: 0.8397\n",
            "Epoch 8/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.3789 - accuracy: 0.8696\n",
            "Epoch 9/25\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.3076 - accuracy: 0.8932\n",
            "Epoch 10/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.2508 - accuracy: 0.9125\n",
            "Epoch 11/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1935 - accuracy: 0.9317\n",
            "Epoch 12/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1577 - accuracy: 0.9458\n",
            "Epoch 13/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1262 - accuracy: 0.9559\n",
            "Epoch 14/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.1047 - accuracy: 0.9636\n",
            "Epoch 15/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0973 - accuracy: 0.9654\n",
            "Epoch 16/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0799 - accuracy: 0.9721\n",
            "Epoch 17/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0744 - accuracy: 0.9745\n",
            "Epoch 18/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0720 - accuracy: 0.9752\n",
            "Epoch 19/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0655 - accuracy: 0.9777\n",
            "Epoch 20/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0660 - accuracy: 0.9769\n",
            "Epoch 21/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0638 - accuracy: 0.9782\n",
            "Epoch 22/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0505 - accuracy: 0.9832\n",
            "Epoch 23/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0646 - accuracy: 0.9773\n",
            "Epoch 24/25\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0580 - accuracy: 0.9792\n",
            "Epoch 25/25\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0539 - accuracy: 0.9812\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               524416    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 591,274\n",
            "Trainable params: 591,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0469 - accuracy: 0.9837\n",
            "Model Accuracy BASE is 0.9837200045585632\n",
            "Setting weight to zero for filter 3\n",
            "Setting weight to zero for filter 13\n",
            "Setting weight to zero for filter 21\n",
            "Setting weight to zero for filter 26\n",
            "Setting weight to zero for filter 47\n",
            "Setting weight to zero for filter 48\n",
            "Setting weight to zero for filter 55\n",
            "Setting weight to zero for filter 61\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0470 - accuracy: 0.9837\n",
            "Model Accuracy AFTER ENTROPY  is 0.9836999773979187\n",
            "Setting weight to zero for filter 2\n",
            "Setting weight to zero for filter 3\n",
            "Setting weight to zero for filter 6\n",
            "Setting weight to zero for filter 7\n",
            "Setting weight to zero for filter 8\n",
            "Setting weight to zero for filter 10\n",
            "Setting weight to zero for filter 11\n",
            "Setting weight to zero for filter 13\n",
            "Setting weight to zero for filter 15\n",
            "Setting weight to zero for filter 19\n",
            "Setting weight to zero for filter 21\n",
            "Setting weight to zero for filter 26\n",
            "Setting weight to zero for filter 31\n",
            "Setting weight to zero for filter 46\n",
            "Setting weight to zero for filter 47\n",
            "Setting weight to zero for filter 48\n",
            "Setting weight to zero for filter 55\n",
            "Setting weight to zero for filter 61\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2535 - accuracy: 0.9245\n",
            "Model Accuracy AFTER SIMILARIY is 0.9245200157165527\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1260 - accuracy: 0.9566\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0442 - accuracy: 0.9848\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.0469 - accuracy: 0.9840\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0508 - accuracy: 0.9827\n",
            "Model Loss is 0.05077357590198517\n",
            "Model Accuracy is 0.9827399849891663\n",
            "Print model size and other info\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               524416    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 591,274\n",
            "Trainable params: 591,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import datasets,models,layers\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
        "\n",
        "\n",
        "####################################################################\n",
        "################   HELPER FUNCTIONS.  ##############################\n",
        "####################################################################\n",
        "\n",
        "\n",
        "#Calculate the entopy of the filter outputs\n",
        "def filter_average_pooled_entropy(average_pooled_filter_output):\n",
        "  # Find the number of classes in this distribution \n",
        "  num_channels   = average_pooled_filter_output.shape[1]\n",
        "  num_examples = average_pooled_filter_output.shape[0]\n",
        "  \n",
        "  entropy_channel = np.zeros(num_channels)\n",
        "\n",
        "  # Measure entropy per channel \n",
        "  for channel_idx in range(0, num_channels, 1):    \n",
        "    hist_f1, bins_f1 = np.histogram(average_pooled_filter_output[ :,channel_idx ], bins=10)\n",
        "    classes_f1       = np.digitize(average_pooled_filter_output[ :,channel_idx], bins_f1)\n",
        "    bin_count        = np.bincount(classes_f1) + 1 \n",
        "    bin_probablities = bin_count/(num_examples)\n",
        "      \n",
        "    # Find the log of each probablity \n",
        "    bin_probablities = bin_probablities  \n",
        "    log_bin_probablities = np.log(bin_probablities)\n",
        "\n",
        "    # Shannon probablity is  Sum (-( p(x)) * log (p(x)))\n",
        "    bin_probablities_product = -1 * np.multiply(log_bin_probablities, bin_probablities) \n",
        "    \n",
        "    #Sum across all classes \n",
        "    entropy_channel[channel_idx] = np.sum(bin_probablities_product)\n",
        "    \n",
        "  return (entropy_channel)\n",
        "\n",
        "\n",
        "# Calculate the variance of the output per batch\n",
        "def batch_variance(outputs):\n",
        "  return (tf.math.reduce_variance(outputs, axis=0))\n",
        "\n",
        "####################################################################\n",
        "##########   BASE MODEL    #########################################\n",
        "####################################################################\n",
        "\n",
        "\n",
        "# Load the CIFAR10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Convert the labels to categorical data\n",
        "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define a VGG model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train_cat, epochs=25, batch_size=128)\n",
        "print(model.summary())\n",
        "\n",
        "# Test the accuracy of the base model \n",
        "ModelLoss, ModelAccuracy = model.evaluate(x_train, y_train_cat)\n",
        "\n",
        "print('Model Accuracy BASE is {}'.format(ModelAccuracy))\n",
        "\n",
        "\n",
        "####################################################################\n",
        "######### PART 1 - MEASURE ENTRORY OF THE FILTER AND PRUNE #########\n",
        "####################################################################\n",
        "\n",
        "# Create an extactor model \n",
        "extractor = keras.Model(inputs=model.inputs, outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "# Create a numpy array with dimentions (num_example, num_hidden_layer_channels)\n",
        "hidden_layer_average_pool = np.zeros((len(x_train),64), dtype=float)\n",
        "batch_output_entropy = []\n",
        "\n",
        "# For the entire training data, we need to find the filter outputs \n",
        "# Create a list per filter (2-D numpy array) of the average values of the filter outputs \n",
        "# For this version, we are only pruning the layer 4 \n",
        "for batch_idx in range(0, (len(x_train)), 25):\n",
        "  # Measure the mutual information between each filter   \n",
        "  first_conv_layer_output = extractor(x_train[batch_idx:batch_idx+25])\n",
        "  first_conv_layer_output_average_pool = tf.reduce_mean(first_conv_layer_output[4], axis=[1, 2])\n",
        "  hidden_layer_average_pool[batch_idx:batch_idx+25] = first_conv_layer_output_average_pool\n",
        "\n",
        "# Measure the entropy of the channel\n",
        "hidden_layer_average_pool_entropy = filter_average_pooled_entropy(hidden_layer_average_pool)\n",
        "\n",
        "filters_pruned = []\n",
        "filters_already_processed = []\n",
        "\n",
        "# Remove all nodes that have low entropy s\n",
        "for hidden_node in range(0, hidden_layer_average_pool.shape[1]):\n",
        "  if( hidden_layer_average_pool_entropy[hidden_node] < 0.4):\n",
        "      print(\"Setting weight to zero for filter\", hidden_node)\n",
        "      \n",
        "      layer = model.layers[4]\n",
        "      W_matrix = layer.weights[0]\n",
        "      b_list = layer.weights[1]\n",
        "\n",
        "      # Set the weights of the W matrix as 0 \n",
        "      mask_b = np.ones(b_list.shape)\n",
        "      mask_b[hidden_node] = 0\n",
        "      mask_b_tensor  = tf.convert_to_tensor(mask_b, dtype=float)\n",
        "      new_b_list     = tf.math.multiply(mask_b_tensor, b_list)\n",
        "\n",
        "      # Set the bias term as 0\n",
        "      mask = np.ones(W_matrix.shape)\n",
        "      mask[:,:,:,hidden_node] = 0\n",
        "      mask_tensor  = tf.convert_to_tensor(mask, dtype=float)\n",
        "      new_W_matrix = tf.math.multiply(mask_tensor, W_matrix)\n",
        "\n",
        "      # Assign new weights to the model \n",
        "      new_weights = [new_W_matrix, new_b_list]\n",
        "      model.layers[4].set_weights(new_weights)\n",
        "\n",
        "      filters_already_processed.append(hidden_node)\n",
        "      filters_pruned.append(hidden_node)\n",
        "\n",
        "ModelLoss, ModelAccuracy = model.evaluate(x_train, y_train_cat)\n",
        "\n",
        "print('Model Accuracy AFTER ENTROPY  is {}'.format(ModelAccuracy))\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "###### PART 2 - PERFORM SIMILARITY MAPPING AND PRUNE ###############\n",
        "####################################################################\n",
        "\n",
        "# Measure the mutual information between all the filters \n",
        "similarity_mapping = np.zeros((hidden_layer_average_pool.shape[1], hidden_layer_average_pool.shape[1]), dtype=float)\n",
        "\n",
        "\n",
        "for f1_idx in range(0, (hidden_layer_average_pool.shape[1]), 1):\n",
        "  for f2_idx in range(f1_idx, (hidden_layer_average_pool.shape[1]), 1):  \n",
        "    corr, p_value = pearsonr(hidden_layer_average_pool[ :,f2_idx ], hidden_layer_average_pool[ :,f1_idx ])\n",
        "    similarity_mapping[f1_idx, f2_idx] = abs(corr)\n",
        "\n",
        "\n",
        "for pruning_index in range(0, 10, 1):  \n",
        "  max_similarity = 0 ;\n",
        "  max_similarity_filters_index  = []\n",
        "  for f1_idx in range(0, (hidden_layer_average_pool.shape[1]), 1):\n",
        "    for f2_idx in range(f1_idx, (hidden_layer_average_pool.shape[1]), 1):\n",
        "      if (similarity_mapping[f1_idx, f2_idx] > max_similarity):\n",
        "        if(f1_idx != f2_idx):\n",
        "          if ((f1_idx not in filters_already_processed) and (f2_idx not in filters_already_processed)):\n",
        "            max_similarity =  similarity_mapping[f1_idx, f2_idx]\n",
        "            max_similarity_filters_index.clear()\n",
        "            max_similarity_filters_index.append(f1_idx)\n",
        "            max_similarity_filters_index.append(f2_idx)\n",
        " \n",
        "  if(max_similarity != 0) : \n",
        "    filters_already_processed.append(max_similarity_filters_index[0])\n",
        "    filters_already_processed.append(max_similarity_filters_index[1])\n",
        "    filters_pruned.append(max_similarity_filters_index[0])\n",
        "\n",
        "# Prune away the one of the filters that are similar \n",
        "for hidden_node in range(0, hidden_layer_average_pool.shape[1]):\n",
        "  if( hidden_node in  filters_pruned):\n",
        "      print(\"Setting weight to zero for filter\", hidden_node)\n",
        "      layer = model.layers[4]\n",
        "      W_matrix = layer.weights[0]\n",
        "      b_list = layer.weights[1]\n",
        "\n",
        "      # Set the weights of the W matrix as 0 \n",
        "      mask_b = np.ones(b_list.shape)\n",
        "      mask_b[hidden_node] = 0\n",
        "      mask_b_tensor  = tf.convert_to_tensor(mask_b, dtype=float)\n",
        "      new_b_list     = tf.math.multiply(mask_b_tensor, b_list)\n",
        "\n",
        "      # Set the bias term as 0\n",
        "      mask = np.ones(W_matrix.shape)\n",
        "      mask[:,:,:,hidden_node] = 0\n",
        "      mask_tensor  = tf.convert_to_tensor(mask, dtype=float)\n",
        "      new_W_matrix = tf.math.multiply(mask_tensor, W_matrix)\n",
        "\n",
        "      # Assign new weights to the model \n",
        "      new_weights = [new_W_matrix, new_b_list]\n",
        "      model.layers[4].set_weights(new_weights)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "################  COLLECT ACCURACY AFTER PRUNING  ################## \n",
        "####################################################################\n",
        "\n",
        "ModelLoss, ModelAccuracy = model.evaluate(x_train, y_train_cat)\n",
        "print('Model Accuracy AFTER SIMILARIY is {}'.format(ModelAccuracy))\n",
        "model.fit(x_train, y_train_cat, epochs=3, batch_size=128)\n",
        "ModelLoss, ModelAccuracy = model.evaluate(x_train, y_train_cat)\n",
        "\n",
        "print('Model Loss is {}'.format(ModelLoss))\n",
        "print('Model Accuracy is {}'.format(ModelAccuracy))\n",
        "print('Print model size and other info')\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO2V-zrd0Jig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}